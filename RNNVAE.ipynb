{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'?', 'S', 'E', 'o', '[', '}', 'B', 'b', ']', 'X', ';', '-', 'Q', '<', '{', '>', 'x'}\n"
     ]
    }
   ],
   "source": [
    "mario_path_files = !ls TheVGLC/Super\\ Mario\\ Bros/Paths/*txt\n",
    "levels = []\n",
    "vocab = set([';','{','}'])\n",
    "for file in mario_path_files:\n",
    "    with open(file) as input_file:\n",
    "        level = []\n",
    "        for row in input_file:\n",
    "            level.append(list(row.rstrip()))\n",
    "            vocab |= set(row.rstrip())\n",
    "        levels.append(level)\n",
    "print(vocab)\n",
    "\n",
    "v2i = {v:i for i,v in enumerate(sorted(vocab))}\n",
    "i2v = {i:v for v,i in v2i.items()}\n",
    "\n",
    "t_levels = []\n",
    "for level in levels:\n",
    "    t_level = []\n",
    "    for row in range(len(level[0])):\n",
    "        column = []\n",
    "        for col in range(len(level)):\n",
    "            column.append(level[col][row])\n",
    "        t_level.append(column)\n",
    "    t_levels.append(t_level)\n",
    "            \n",
    "chunks = []\n",
    "chunk_size = 32\n",
    "for level in t_levels:\n",
    "    for x in range(len(level)-chunk_size):\n",
    "        chunk = level[x:x+chunk_size]\n",
    "        chunk = [''.join(c) for c in chunk]\n",
    "        chunks.append(';'.join(chunk)+'}')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNVAE(nn.Module):\n",
    "    def __init__(self,e_layers,d_layers,\n",
    "                 dropout,vocab_size,\n",
    "                 hidden_size,latent_size,\n",
    "                 rnn_type):\n",
    "        super(RNNVAE, self).__init__()\n",
    "        self.latent_dim = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.encoder = rnn_type(hidden_size,hidden_size=hidden_size,\n",
    "                              num_layers=e_layers,batch_first=True,\n",
    "                                dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        self.to_mu = nn.Linear(hidden_size*2,latent_size)\n",
    "        self.to_logvar = nn.Linear(hidden_size*2,latent_size)\n",
    "        self.latent_to_h = nn.Linear(latent_size,hidden_size*d_layers)\n",
    "        self.latent_to_c = nn.Linear(latent_size,hidden_size*d_layers)\n",
    "        \n",
    "        self.decoder = rnn_type(hidden_size,hidden_size=hidden_size,\n",
    "                              num_layers=d_layers,dropout=dropout,\n",
    "                                batch_first=True,)\n",
    "        self.hidden_to_vocab = nn.Linear(hidden_size,vocab_size)\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "    def forward(self,input_sequence,start_tok):\n",
    "        \n",
    "        batch_size, seq_length = input_sequence.size(0), input_sequence.size(1)\n",
    "        device = input_sequence.device\n",
    "        embedded = self.embed(input_sequence)\n",
    "        out , _= self.encoder(embedded)\n",
    "        \n",
    "        f_enc = out[:,0,:out.size()[-1]//2]\n",
    "        r_enc = out[:,-1,out.size()[-1]//2:]\n",
    "        enc = torch.cat((f_enc,r_enc),dim=-1)\n",
    "        mu,logvar = self.to_mu(enc), self.to_logvar(enc)\n",
    "        \n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = torch.randn([batch_size,self.latent_dim],device=device)\n",
    "        z = z * std + mu        \n",
    "        dec_h = self.latent_to_h(z)\n",
    "        dec_h = dec_h.view(batch_size,-1, self.hidden_size).permute(1,0,2).contiguous()\n",
    "        dec_c = self.latent_to_c(z)\n",
    "        dec_c = dec_c.view(batch_size,-1, self.hidden_size).permute(1,0,2).contiguous()\n",
    "\n",
    "        dec_inp = torch.cat((\n",
    "             self.embed(torch.ones(batch_size,1,dtype=torch.long,device=device)*start_tok),\n",
    "            embedded[:,:-1,:]),dim=1)        \n",
    "        \n",
    "        dec_out, _ =self.decoder(dec_inp,(dec_h,dec_c))\n",
    "        dec_logit = self.hidden_to_vocab(dec_out)\n",
    "        \n",
    "        loss = self.CE(dec_logit.view(batch_size*seq_length,-1),input_sequence.view(-1))\n",
    "        KL_div =  -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return loss, KL_div, dec_logit\n",
    "     \n",
    "rnn_vae = RNNVAE(2,4,0.5,len(vocab),512,32,nn.LSTM)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ee26b8420842cdb4ceeee3a2885772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd132f7313d4cd6877847a904424a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=77), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13879427104257047 0.13879427104257047 0.013472948223352432\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import numpy as np\n",
    "device = 'cuda'\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "show_every = 16\n",
    "rnn_vae.to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_vae.parameters(),lr=1e-4)\n",
    "losses = []\n",
    "annealing = 1.0\n",
    "annealing_rate = 0.999\n",
    "for epoch in tqdm_notebook(range(epochs)):\n",
    "    random.shuffle(chunks)\n",
    "    for batch in tqdm_notebook(range(0,len(chunks),batch_size)):\n",
    "        rnn_vae.train()\n",
    "        rnn_vae.zero_grad()\n",
    "        batch = chunks[batch:batch+batch_size]\n",
    "        batch = [[v2i[t] for t in c] for c in batch]\n",
    "        input_sequence =torch.tensor(batch).to(device)\n",
    "        loss, KL_div, dec_logit = rnn_vae(input_sequence,v2i['{'])\n",
    "        loss_KL_div = loss + KL_div*(1.0-annealing)\n",
    "        losses.append((loss_KL_div.item(),loss.item(),KL_div.item()))\n",
    "        loss_KL_div.backward()\n",
    "        optimizer.step()\n",
    "        if len(losses) % show_every == 0:\n",
    "            losses_ = np.array(losses)\n",
    "            print(np.mean(losses_[-show_every*2:,0]),\n",
    "                  np.mean(losses_[-show_every*2:,1]),\n",
    "                  np.mean(losses_[-show_every*2:,2]))\n",
    "    annealing *= annealing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        \n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "def autoencode(self,input_sequence,start_tok):\n",
    "        \n",
    "    batch_size, seq_length = input_sequence.size(0), input_sequence.size(1)\n",
    "    device = input_sequence.device\n",
    "    embedded = self.embed(input_sequence)\n",
    "    out , _= self.encoder(embedded)\n",
    "\n",
    "    f_enc = out[:,0,:out.size()[-1]//2]\n",
    "    r_enc = out[:,-1,out.size()[-1]//2:]\n",
    "    enc = torch.cat((f_enc,r_enc),dim=-1)\n",
    "    mu,logvar = self.to_mu(enc), self.to_logvar(enc)\n",
    "\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    z = torch.randn([batch_size,self.latent_dim],device=device)\n",
    "    z = z * std + mu        \n",
    "    dec_h = self.latent_to_h(z)\n",
    "    dec_h = dec_h.view(batch_size,-1, self.hidden_size).permute(1,0,2).contiguous()\n",
    "    dec_c = self.latent_to_c(z)\n",
    "    dec_c = dec_c.view(batch_size,-1, self.hidden_size).permute(1,0,2).contiguous()\n",
    "\n",
    "    dec_inp = self.embed(torch.ones(batch_size,1,dtype=torch.long,device=device)*start_tok)\n",
    "    tokens = []    \n",
    "    for _ in range(seq_length):\n",
    "        dec_out, _ =self.decoder(dec_inp,(dec_h,dec_c))\n",
    "        dec_logit = self.hidden_to_vocab(dec_out[:,-1,:]).view(batch_size,-1)\n",
    "        dec_logit = top_k_top_p_filtering(dec_logit,top_p = 0.9)\n",
    "        next_token = torch.multinomial(F.softmax(dec_logit, dim=-1), num_samples=1)\n",
    "        \n",
    "        tokens.append(next_token.detach())\n",
    "        dec_inp =torch.cat((dec_inp,self.embed(next_token)\n",
    "            ),dim=1)\n",
    "        \n",
    "\n",
    "    return tokens\n",
    "\n",
    "rnn_vae.eval()\n",
    "encoded = autoencode(rnn_vae,input_sequence,v2i['{'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "-----Q---Q--xX\n",
      "------------xX\n",
      "-----Q---Q--xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "-----------xxX}\n",
      "------\n",
      "----------xx-X\n",
      "---------xx--X\n",
      "---------x<[[X\n",
      "--------xx>]]X\n",
      "--------x----X\n",
      "--------x---EX\n",
      "--------x----X\n",
      "--------x----X\n",
      "---------x-<[X\n",
      "----------x>]X\n",
      "----------x--X\n",
      "-----------x-X\n",
      "------------xX\n",
      "------------xX\n",
      "------------xX\n",
      "-----------xxX\n",
      "----------xx-X\n",
      "---------xx--X\n",
      "--------xx---X\n",
      "--------x<[[[X\n",
      "-------xx>]]]X\n",
      "-------x-----X\n",
      "-------x-----X\n",
      "-------x-----X\n",
      "-------x-S---X\n",
      "-------xxS---X\n",
      "-------x-----X\n",
      "------xx------\n",
      "-----xxX------\n",
      "----xx--X-----\n",
      "---xx---------\n",
      "--xx----------}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generation = []\n",
    "b = 4\n",
    "for batch in encoded:\n",
    "    token = batch[b].item()\n",
    "    generation.append(token)\n",
    "\n",
    "print(''.join([i2v[t.item()] for t in input_sequence[b,:]]).replace(';','\\n'))\n",
    "print('------')\n",
    "print(''.join([i2v[t] for t in generation]).replace(';','\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
